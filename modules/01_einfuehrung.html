<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EinfÃ¼hrung - Wie funktioniert ein LLM?</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ¤– Wie funktioniert ein Large Language Model (LLM)?</h1>
            <p>Eine interaktive ErklÃ¤rung am Beispiel</p>
        </header>

        <nav>
            <a href="../index.html">ğŸ  Ãœbersicht</a>
            <a href="01_einfuehrung.html" class="active">1. EinfÃ¼hrung</a>
            <a href="02_tokenisierung.html">2. Tokenisierung</a>
            <a href="03_embeddings.html">3. Embeddings</a>
        </nav>

        <div class="card fade-in">
            <h2>Was ist ein Large Language Model?</h2>

            <p>
                Ein Large Language Model (LLM) wie GPT generiert Text, indem es den Eingabekontext analysiert.
            </p>

            <p>
                Basierend auf antrainierten <strong>Gewichten (Parametern)</strong> gibt das Modell dann
                das Folgewort mit der hÃ¶chsten Wahrscheinlichkeit aus.
            </p>

            <p>
                <strong>Wie funktioniert die Vorhersage?</strong><br>
                Die Vorhersage stÃ¼tzt sich auf Gewichtungen, die das Modell wÃ¤hrend des Trainings gelernt hat.
            </p>

            <p>
                Diese Gewichtungen reprÃ¤sentieren Beziehungen zwischen WÃ¶rtern und Phrasen.
                Sie wurden aus einem groÃŸen Korpus an Textdaten erlernt.
            </p>

            <p>
                Das Modell bestimmt damit, welche WÃ¶rter als Folge des Eingabekontextes
                die hÃ¶chste Wahrscheinlichkeit haben.
            </p>

            <p>
                Dabei werden nicht nur einzelne WÃ¶rter betrachtet, sondern auch ganze Satzstrukturen und Kontexte.
                So entstehen <strong>kohÃ¤rente</strong> (= stimmige, logische) und relevante Textausgaben.
            </p>

            <div class="info-box">
                <h4>ğŸ’¡ Was bedeutet "kohÃ¤rent"?</h4>
                <p>
                    â€KohÃ¤rent" beschreibt etwas, das <strong>stimmig, logisch und einheitlich strukturiert</strong> ist.
                    Ein kohÃ¤renter Text ergibt Sinn und folgt einer nachvollziehbaren Logik.
                </p>
            </div>
        </div>

        <div class="card fade-in">
            <h2>Unser Beispielsatz</h2>

            <p>
                Um zu verstehen, wie ein LLM funktioniert, werden wir in den folgenden Modulen
                Schritt fÃ¼r Schritt analysieren, wie das Modell mit diesem Beispielsatz umgeht:
            </p>

            <div class="example-sentence">
                Welche Farbe hat der Himmel?
            </div>

            <p>
                Anhand dieses einfachen Satzes werden wir folgende Schritte durchlaufen:
            </p>

            <ol style="font-size: 1.1em; line-height: 2;">
                <li><strong>Tokenisierung:</strong> Wie wird der Satz in einzelne Bausteine (Tokens) zerlegt?</li>
                <li><strong>Embeddings:</strong> Wie werden diese Tokens in numerische Vektoren umgewandelt?</li>
                <li><strong>Attention:</strong> Wie erkennt das Modell, welche WÃ¶rter zusammengehÃ¶ren?</li>
                <li><strong>Transformer:</strong> Wie verarbeitet das Modell die Information?</li>
                <li><strong>Vorhersage:</strong> Wie berechnet das Modell Wahrscheinlichkeiten fÃ¼r das nÃ¤chste Wort?</li>
            </ol>
        </div>

        <div class="card fade-in">
            <h2>Der grundlegende Ablauf</h2>

            <div class="interactive-box">
                <h4>Vereinfachter Prozess:</h4>

                <div style="font-size: 1.2em; line-height: 2.5; padding: 20px;">
                    ğŸ“ <strong>Eingabe:</strong> "Welche Farbe hat der Himmel?"
                    <br>
                    â¬‡ï¸
                    <br>
                    ğŸ”¤ <strong>Tokenisierung:</strong> ["Welche", "Farbe", "hat", "der", "Himmel", "?"]
                    <br>
                    â¬‡ï¸
                    <br>
                    ğŸ”¢ <strong>Embeddings:</strong> Jedes Token wird zu einem Zahlenvektor
                    <br>
                    â¬‡ï¸
                    <br>
                    ğŸ§  <strong>Transformer:</strong> Neuronales Netzwerk verarbeitet die Vektoren
                    <br>
                    â¬‡ï¸
                    <br>
                    ğŸ“Š <strong>Wahrscheinlichkeiten:</strong> Berechnung fÃ¼r mÃ¶gliche FolgewÃ¶rter
                    <br>
                    â¬‡ï¸
                    <br>
                    âœ… <strong>Ausgabe:</strong> Das wahrscheinlichste Wort (z.B. "blau")
                </div>
            </div>
        </div>

        <div class="card fade-in">
            <h2>Vorschau auf die nÃ¤chsten Schritte</h2>

            <p>
                Im nÃ¤chsten Modul (<strong>Tokenisierung</strong>) werden wir uns genau ansehen,
                wie unser Beispielsatz in einzelne Tokens zerlegt wird und warum dies ein wichtiger
                erster Schritt ist.
            </p>

            <div class="info-box warning">
                <h4>âš ï¸ Wichtig zu verstehen:</h4>
                <p>
                    Ein LLM "versteht" Text nicht im menschlichen Sinne. Es erkennt Muster und
                    statistische ZusammenhÃ¤nge aus den Trainingsdaten. Die Ausgabe basiert auf
                    Wahrscheinlichkeiten, nicht auf echtem VerstÃ¤ndnis oder Bewusstsein.
                </p>
            </div>

            <div style="text-align: center; margin-top: 30px;">
                <a href="02_tokenisierung.html" class="btn">
                    Weiter zur Tokenisierung â†’
                </a>
            </div>
        </div>

        <footer>
            <p>LLM ErklÃ¤rung - Modul 1: EinfÃ¼hrung | Navigation: <a href="02_tokenisierung.html">NÃ¤chstes Modul â†’</a></p>
        </footer>
    </div>
</body>
</html>
