<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention is All You Need - Wie funktioniert ein LLM?</title>
    <link rel="stylesheet" href="../css/style.css">
    <style>
        .iframe-wrapper {
            background: white;
            border-radius: 12px;
            border: 2px solid var(--border-color);
            box-shadow: var(--shadow);
            padding: 15px;
        }

        .module-iframe {
            width: 100%;
            min-height: 820px;
            border: none;
            border-radius: 8px;
        }

        @media (max-width: 768px) {
            .module-iframe {
                min-height: 960px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ§  Attention is All You Need</h1>
            <p>Self-Attention macht den Unterschied â€“ so findet das Modell relevante WÃ¶rter fÃ¼r â€Welche Farbe hat der Himmel?â€œ</p>
        </header>

        <nav>
            <a href="../index.html">ğŸ  Ãœbersicht</a>
            <a href="01_einfuehrung.html">1. EinfÃ¼hrung</a>
            <a href="02_tokenisierung.html">2. Tokenisierung</a>
            <a href="03_embeddings.html">3. Embeddings</a>
            <a href="04_attention.html" class="active">4. Attention</a>
            <a href="05_softmax.html">5. Softmax</a>
        </nav>

        <div class="card fade-in">
            <h2>Warum Self-Attention â€allesâ€œ ist</h2>
            <p>
                Attention ersetzt klassische rekurrente Schritte: Statt Position fÃ¼r Position zu laufen,
                vergleicht das Modell jedes Token parallel mit allen anderen und vergibt Gewichte. Ein
                hoher Wert bedeutet: â€Dieses Wort liefert mir Kontextâ€œ. FÃ¼r unseren Satz heiÃŸt das,
                dass <strong>â€Himmelâ€œ</strong> auf WÃ¶rter wie <strong>â€Farbeâ€œ</strong> achtet, wÃ¤hrend
                FunktionwÃ¶rter wie <strong>â€derâ€œ</strong> weniger Gewicht bekommen.
            </p>
            <div class="info-box">
                <h4>ğŸ’¡ Merksatz</h4>
                <p>
                    Attention = <em>Ã„hnlichkeit</em> zwischen einem Query (aktuelles Wort) und allen Keys (andere WÃ¶rter).
                    Das Ergebnis (Softmax-Gewichte) wird mit den Value-Vektoren kombiniert und erzeugt eine
                    kontextreiche Darstellung fÃ¼r das nÃ¤chste Netzteil.
                </p>
            </div>
        </div>

        <div class="card fade-in">
            <h2>Der Prozess in fÃ¼nf Schritten</h2>
            <ol style="font-size: 1.1em; line-height: 2;">
                <li><strong>Embeddings laden:</strong> Token-Vektoren stammen aus Modul 3.</li>
                <li><strong>Q, K, V berechnen:</strong> Matrixmultiplikation mit gelernten Gewichten.</li>
                <li><strong>Skalierte Punktprodukte:</strong> <code>Q Â· Káµ€ / âˆšdâ‚–</code> misst die Relevanz.</li>
                <li><strong>Softmax anwenden:</strong> Scores werden zu Wahrscheinlichkeiten (siehe Modul 5).</li>
                <li><strong>Gewichtete Summe:</strong> Werte (V) werden gemÃ¤ÃŸ Gewicht addiert â†’ neues Kontext-Embedding.</li>
            </ol>
            <div class="info-box warning">
                <h4>âš ï¸ Tipp fÃ¼r Trainer:innen</h4>
                <p>
                    Lass Teilnehmende zuerst manuell Ã¼berlegen, welches Wort fÃ¼r â€Himmelâ€œ wichtig ist,
                    bevor sie die Animation starten. Das stÃ¤rkt das VerstÃ¤ndnis fÃ¼r die Gewichtung.
                </p>
            </div>
        </div>

        <div class="card fade-in">
            <h2>Interaktive Visualisierung</h2>
            <p>
                Die folgende Einbettung lÃ¤dt das ausfÃ¼hrliche Visualisierungstool. Du kannst Tokens auswÃ¤hlen,
                Schritt fÃ¼r Schritt durch Q/K/V gehen und beobachten, wie die Softmax-Gewichte entstehen.
            </p>
            <div class="iframe-wrapper">
                <iframe
                    src="attention_mechanism.html"
                    title="Interaktive Attention-Animation"
                    class="module-iframe"
                    loading="lazy"
                ></iframe>
            </div>
            <p style="margin-top: 15px; font-size: 0.95em; color: #475569;">
                ğŸ’» Hinweis: FÃ¼r das beste Erlebnis im Vollbild Ã¶ffnen oder direkt in einem neuen Tab starten.
            </p>
        </div>

        <div class="card fade-in">
            <h2>Worauf du achten solltest</h2>
            <ul style="line-height: 1.9; font-size: 1.05em;">
                <li>Die Summe der Attention-Gewichte pro Schritt ist immer 1 (Softmax-Eigenschaft).</li>
                <li>Selbst unscheinbare WÃ¶rter kÃ¶nnen wichtig werden, falls sie in anderen Kontexten relevant sind.</li>
                <li>Multi-Head Attention wiederholt diesen Ablauf parallel mit unterschiedlichen Gewichtsmatrizen.</li>
            </ul>
        </div>

        <div style="text-align: center; margin-top: 30px; display: flex; gap: 15px; justify-content: center;">
            <a href="03_embeddings.html" class="btn btn-secondary">
                â† ZurÃ¼ck zu Embeddings
            </a>
            <a href="05_softmax.html" class="btn">
                Weiter zu Softmax â†’
            </a>
        </div>

        <footer style="margin-top: 40px;">
            <p>LLM ErklÃ¤rung - Modul 4: Attention is All You Need | Beispielsatz: "Welche Farbe hat der Himmel?"</p>
        </footer>
    </div>
</body>
</html>
