<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Erkl√§rung - Wie funktioniert ein Large Language Model?</title>
    <link rel="stylesheet" href="css/style.css">
    <style>
        /* Modern Color Palette for Modules */
        :root {
            --gradient-1: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            --gradient-2: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            --gradient-3: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
            --gradient-4: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
            --gradient-5: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            --gradient-6: linear-gradient(135deg, #30cfd0 0%, #330867 100%);
            --gradient-7: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            --gradient-8: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            --gradient-9: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            --gradient-10: linear-gradient(135deg, #ff6e7f 0%, #bfe9ff 100%);
        }

        .module-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(360px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .module-card {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            cursor: pointer;
            text-decoration: none;
            color: inherit;
            display: block;
            position: relative;
            overflow: hidden;
            border: 2px solid transparent;
        }

        .module-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 6px;
            background: var(--card-gradient);
            transition: height 0.4s ease;
        }

        .module-card:hover::before {
            height: 100%;
            opacity: 0.05;
        }

        .module-card:hover {
            transform: translateY(-8px) scale(1.02);
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.12);
            border-color: rgba(102, 126, 234, 0.2);
        }

        .module-card h3 {
            color: #1e293b;
            font-size: 1.75em;
            margin-bottom: 18px;
            display: flex;
            align-items: center;
            gap: 15px;
            font-weight: 700;
            line-height: 1.3;
        }

        .module-card .module-number {
            background: var(--card-gradient);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 800;
            font-size: 1.3em;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.15);
            flex-shrink: 0;
        }

        .module-card p {
            color: #475569;
            line-height: 1.8;
            font-size: 1.1em;
            margin-bottom: 20px;
        }

        .module-card .module-status {
            margin-top: 20px;
            padding: 10px 20px;
            background: rgba(102, 126, 234, 0.1);
            border-radius: 10px;
            font-size: 0.95em;
            font-weight: 700;
            display: inline-block;
        }

        .module-card.completed .module-status {
            background: rgba(16, 185, 129, 0.1);
            color: #059669;
        }

        .module-card.coming-soon {
            opacity: 0.7;
            cursor: not-allowed;
        }

        .module-card.coming-soon:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);
        }

        /* Individual module colors */
        .module-card:nth-child(1) { --card-gradient: var(--gradient-1); }
        .module-card:nth-child(2) { --card-gradient: var(--gradient-2); }
        .module-card:nth-child(3) { --card-gradient: var(--gradient-3); }
        .module-card:nth-child(4) { --card-gradient: var(--gradient-4); }
        .module-card:nth-child(5) { --card-gradient: var(--gradient-5); }
        .module-card:nth-child(6) { --card-gradient: var(--gradient-6); }
        .module-card:nth-child(7) { --card-gradient: var(--gradient-7); }
        .module-card:nth-child(8) { --card-gradient: var(--gradient-8); }
        .module-card:nth-child(9) { --card-gradient: var(--gradient-9); }
        .module-card:nth-child(10) { --card-gradient: var(--gradient-10); }

        .hero-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 70px 50px;
            border-radius: 25px;
            margin-bottom: 50px;
            text-align: center;
            box-shadow: 0 20px 60px rgba(102, 126, 234, 0.3);
            position: relative;
            overflow: hidden;
        }

        .hero-section::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255, 255, 255, 0.1) 0%, transparent 70%);
            animation: pulse 15s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); opacity: 0.5; }
            50% { transform: scale(1.1); opacity: 0.8; }
        }

        .hero-section h1 {
            font-size: 3.5em;
            margin-bottom: 25px;
            font-weight: 800;
            text-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
            position: relative;
            z-index: 1;
        }

        .hero-section > p {
            font-size: 1.5em;
            opacity: 0.98;
            max-width: 800px;
            margin: 0 auto;
            line-height: 1.6;
            position: relative;
            z-index: 1;
        }

        .example-preview {
            background: rgba(255, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            padding: 25px 35px;
            border-radius: 15px;
            margin-top: 40px;
            font-size: 1.8em;
            font-weight: 800;
            border: 2px solid rgba(255, 255, 255, 0.3);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            position: relative;
            z-index: 1;
        }

        /* Enhanced card sections */
        .card {
            background: white;
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 35px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.06);
            border: 1px solid rgba(0, 0, 0, 0.05);
        }

        .card h2 {
            color: #1e293b;
            margin-bottom: 25px;
            font-size: 2.2em;
            font-weight: 800;
            line-height: 1.3;
        }

        .card h2::before {
            content: '';
            display: inline-block;
            width: 6px;
            height: 40px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            margin-right: 15px;
            border-radius: 3px;
            vertical-align: middle;
        }

        .info-box {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.05), rgba(118, 75, 162, 0.05));
            border-left: 5px solid #667eea;
            padding: 30px;
            margin: 30px 0;
            border-radius: 15px;
            font-size: 1.1em;
            line-height: 1.9;
        }

        .info-box h4 {
            color: #667eea;
            font-size: 1.4em;
            font-weight: 700;
            margin-bottom: 20px;
        }

        .info-box ul {
            list-style: none;
            padding-left: 0;
        }

        .info-box ul li {
            padding-left: 35px;
            position: relative;
            margin-bottom: 15px;
        }

        .info-box ul li::before {
            content: '‚úì';
            position: absolute;
            left: 0;
            color: #667eea;
            font-weight: bold;
            font-size: 1.3em;
        }

        /* Responsive improvements */
        @media (max-width: 768px) {
            .hero-section {
                padding: 40px 30px;
            }

            .hero-section h1 {
                font-size: 2.2em;
            }

            .module-grid {
                grid-template-columns: 1fr;
                gap: 20px;
            }

            .card {
                padding: 30px 25px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="hero-section fade-in">
            <h1>ü§ñ Wie funktioniert ein LLM?</h1>
            <p>
                Eine interaktive Schritt-f√ºr-Schritt Erkl√§rung, wie Large Language Models
                Text verstehen und generieren
            </p>
            <div class="example-preview">
                üìù Unser Beispielsatz: "Welche Farbe hat der Himmel?"
            </div>
        </div>

        <div class="card fade-in">
            <h2>üéØ √úber diese Lernreihe</h2>
            <p style="font-size: 1.1em; line-height: 1.8;">
                Diese interaktive Erkl√§rung zeigt dir Schritt f√ºr Schritt, wie ein Large Language Model (LLM)
                funktioniert. Anhand des konkreten Beispielsatzes <strong>"Welche Farbe hat der Himmel?"</strong>
                werden wir jeden Prozessschritt detailliert durchgehen - von der Tokenisierung bis zur
                finalen Textgenerierung.
            </p>

            <div class="info-box" style="margin-top: 20px;">
                <h4>üí° Was du lernen wirst:</h4>
                <ul style="line-height: 1.8; font-size: 1.05em;">
                    <li>Wie Text in Tokens zerlegt wird (Tokenisierung)</li>
                    <li>Wie Tokens zu numerischen Vektoren werden (Embeddings)</li>
                    <li>Wie das Modell Zusammenh√§nge erkennt (Attention)</li>
                    <li>Wie die Transformer-Architektur funktioniert</li>
                    <li>Wie Wahrscheinlichkeiten f√ºr das n√§chste Wort berechnet werden</li>
                    <li>Wie das Modell trainiert wird</li>
                </ul>
            </div>
        </div>

        <div class="card fade-in">
            <h2>üìö Lernmodule - Grundlagen</h2>
            <p style="font-size: 1.1em; margin-bottom: 20px;">
                Starte mit den Grundlagen und arbeite dich durch die Module.
                Jedes Modul baut auf dem vorherigen auf.
            </p>

            <div class="module-grid">
                <a href="modules/01_einfuehrung.html" class="module-card completed">
                    <h3>
                        <span class="module-number">1</span>
                        Einf√ºhrung
                    </h3>
                    <p>
                        Was ist ein Large Language Model? Grundlegendes Verst√§ndnis und √úberblick
                        √ºber den gesamten Prozess der Textverarbeitung und -generierung.
                    </p>
                    <div class="module-status">‚úÖ Verf√ºgbar</div>
                </a>

                <a href="modules/02_tokenisierung.html" class="module-card completed">
                    <h3>
                        <span class="module-number">2</span>
                        Tokenisierung
                    </h3>
                    <p>
                        Wie wird Text in einzelne Bausteine (Tokens) zerlegt? Verstehe das
                        Vokabular und wie unser Beispielsatz in 6 Tokens aufgeteilt wird.
                    </p>
                    <div class="module-status">‚úÖ Verf√ºgbar</div>
                </a>

                <a href="modules/03_embeddings.html" class="module-card completed">
                    <h3>
                        <span class="module-number">3</span>
                        Embeddings
                    </h3>
                    <p>
                        Von Tokens zu numerischen Vektoren. Lerne, wie jedes Token in einen
                        4-dimensionalen Vektor umgewandelt wird und warum √§hnliche W√∂rter √§hnliche Vektoren haben.
                    </p>
                    <div class="module-status">‚úÖ Verf√ºgbar</div>
                </a>

                <a href="modules/04_attention.html" class="module-card completed">
                    <h3>
                        <span class="module-number">4</span>
                        Attention is All You Need
                    </h3>
                    <p>
                        Warum Self-Attention der Kern des Transformers ist. Beobachte,
                        wie Q-, K- und V-Vektoren unseren Beispielsatz in Kontext setzen
                        und begleite die interaktive Animation.
                    </p>
                    <div class="module-status">‚úÖ Verf√ºgbar</div>
                </a>

                <a href="modules/05_softmax.html" class="module-card completed">
                    <h3>
                        <span class="module-number">5</span>
                        Softmax & Wahrscheinlichkeiten
                    </h3>
                    <p>
                        Vom Logit zur Wahrscheinlichkeit: Lerne, wie Softmax aus Modellwerten
                        eine Verteilung ableitet und wie Temperatur oder Masken sie ver√§ndern.
                    </p>
                    <div class="module-status">‚úÖ Verf√ºgbar</div>
                </a>
            </div>
        </div>

        <div class="card fade-in">
            <h2>üèóÔ∏è Kommende Module</h2>
            <p style="font-size: 1.1em; margin-bottom: 20px;">
                Diese Module sind in Planung und werden bald verf√ºgbar sein:
            </p>

            <div class="module-grid">
                <div class="module-card coming-soon">
                    <h3>
                        <span class="module-number">6</span>
                        Transformer-Architektur
                    </h3>
                    <p>
                        Die Gesamtarchitektur des Modells. Multi-Head Attention, Feed-Forward Networks
                        und wie alles zusammenarbeitet.
                    </p>
                    <div class="module-status">üîú In Vorbereitung</div>
                </div>

                <div class="module-card coming-soon">
                    <h3>
                        <span class="module-number">7</span>
                        Parameter & Gewichte
                    </h3>
                    <p>
                        Was sind die Millionen von Parametern? Wie werden sie gespeichert und
                        warum brauchen gr√∂√üere Modelle mehr Parameter?
                    </p>
                    <div class="module-status">üîú In Vorbereitung</div>
                </div>

                <div class="module-card coming-soon">
                    <h3>
                        <span class="module-number">8</span>
                        Forward Pass
                    </h3>
                    <p>
                        Der komplette Durchlauf: Wie flie√üen die Daten vom Input zum Output?
                        Schritt-f√ºr-Schritt Visualisierung des gesamten Prozesses.
                    </p>
                    <div class="module-status">üîú In Vorbereitung</div>
                </div>

                <div class="module-card coming-soon">
                    <h3>
                        <span class="module-number">9</span>
                        Sampling-Strategien
                    </h3>
                    <p>
                        Temperature, Top-k, Top-p - Wie wird aus Wahrscheinlichkeiten das
                        tats√§chliche n√§chste Wort ausgew√§hlt?
                    </p>
                    <div class="module-status">üîú In Vorbereitung</div>
                </div>

                <div class="module-card coming-soon">
                    <h3>
                        <span class="module-number">10</span>
                        Training & Optimierung
                    </h3>
                    <p>
                        Wie lernt das Modell? Pre-Training, Loss-Funktionen und wie die
                        Gewichte optimiert werden.
                    </p>
                    <div class="module-status">üîú In Vorbereitung</div>
                </div>
            </div>
        </div>

        <div class="card fade-in">
            <h2>üöÄ Jetzt starten!</h2>
            <p style="font-size: 1.1em; margin-bottom: 25px;">
                Bereit, zu verstehen, wie LLMs funktionieren? Beginne mit der Einf√ºhrung:
            </p>

            <div style="text-align: center;">
                <a href="modules/01_einfuehrung.html" class="btn" style="font-size: 1.2em; padding: 15px 35px;">
                    üìñ Zur Einf√ºhrung starten ‚Üí
                </a>
            </div>
        </div>

        <div class="card fade-in">
            <h2>üìä Projektressourcen</h2>
            <div class="info-box">
                <h4>üìÅ Verf√ºgbare Dateien:</h4>
                <ul style="line-height: 1.8;">
                    <li><strong>Vokabular-Daten:</strong> <code>data/vocabulary.json</code> - Vollst√§ndiges W√∂rterbuch mit Token-IDs und Embeddings</li>
                    <li><strong>Styles:</strong> <code>css/style.css</code> - Globale Styles f√ºr alle Module</li>
                    <li><strong>Module:</strong> <code>modules/</code> - Alle Lernmodule (HTML)</li>
                </ul>
            </div>
        </div>

        <footer>
            <p>ü§ñ LLM Erkl√§rung - Interaktive Lernreihe | Beispielsatz: "Welche Farbe hat der Himmel?"</p>
            <p style="margin-top: 10px; font-size: 0.9em; color: #94a3b8;">
                Diese Erkl√§rung verwendet vereinfachte Beispiele (4-dimensionale Embeddings, 30-Token-Vokabular).
                Echte LLMs arbeiten mit deutlich h√∂heren Dimensionen (768+) und gr√∂√üeren Vokabularen (50.000+).
            </p>
        </footer>
    </div>
</body>
</html>
